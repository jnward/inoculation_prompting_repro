[project]
name = "gradient-routing"
version = "0.1.0"
description = "Gradient routing experiments: MBPP code reward hacking, Reddit toxicity, GCD sycophancy"
requires-python = ">=3.11,<3.13"
dependencies = [
    "torch>=2.6.0,<2.7",
    "transformers>=4.44.0,<5.0.0",
    "accelerate>=0.33.0",
    "peft>=0.12.0",
    "trl>=0.9.0",
    "bitsandbytes>=0.43.0",
    "datasets>=2.20.0",
    "simple-parsing>=0.1.5",
    "vllm>=0.8.4",
    "inspect-ai>=0.3.90",
    "requests>=2.32.0",
    "setuptools>=80.10.2",
    "wheel>=0.46.3",
    "flash-attn==2.7.4.post1",
    "openai>=1.78.0",
    "wandb",
    "unidecode>=1.3.0",
    "matplotlib>=3.10.8",
]

[project.optional-dependencies]
# NOTE: Reddit experiment requires safetytooling and unidecode, installed separately
# due to openai version conflict (safetytooling pins openai==1.70.0):
#   uv pip install "safetytooling @ git+https://github.com/safety-research/safety-tooling.git@main"
#   uv pip install unidecode
reddit = [
    "unidecode>=1.3.0",
]
dev = [
    "pytest>=8.0.0",
    "ruff>=0.5.0",
]

[build-system]
requires = ["setuptools>=80.10.2"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
include = ["shared*", "mbpp*", "reddit*", "gcd*"]

[tool.uv]

[tool.uv.extra-build-dependencies]
flash-attn = ["torch"]

[tool.uv.sources]
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl" }
